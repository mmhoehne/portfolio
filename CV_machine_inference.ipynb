{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset Class Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.4.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-cognitiveservices-search-imagesearch\n",
      "  Downloading azure_cognitiveservices_search_imagesearch-2.0.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting msrest>=0.6.21 (from azure-cognitiveservices-search-imagesearch)\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting azure-common~=1.1 (from azure-cognitiveservices-search-imagesearch)\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting azure-mgmt-core<2.0.0,>=1.2.0 (from azure-cognitiveservices-search-imagesearch)\n",
      "  Downloading azure_mgmt_core-1.5.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure-mgmt-core<2.0.0,>=1.2.0->azure-cognitiveservices-search-imagesearch)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mattias.hoehnen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from msrest>=0.6.21->azure-cognitiveservices-search-imagesearch) (2024.2.2)\n",
      "Collecting isodate>=0.6.0 (from msrest>=0.6.21->azure-cognitiveservices-search-imagesearch)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.6.21->azure-cognitiveservices-search-imagesearch)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests~=2.16 in c:\\users\\mattias.hoehnen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from msrest>=0.6.21->azure-cognitiveservices-search-imagesearch) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\mattias.hoehnen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from azure-core>=1.31.0->azure-mgmt-core<2.0.0,>=1.2.0->azure-cognitiveservices-search-imagesearch) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\mattias.hoehnen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from azure-core>=1.31.0->azure-mgmt-core<2.0.0,>=1.2.0->azure-cognitiveservices-search-imagesearch) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mattias.hoehnen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-cognitiveservices-search-imagesearch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mattias.hoehnen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-cognitiveservices-search-imagesearch) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mattias.hoehnen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-cognitiveservices-search-imagesearch) (2.2.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-cognitiveservices-search-imagesearch)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading azure_cognitiveservices_search_imagesearch-2.0.1-py2.py3-none-any.whl (47 kB)\n",
      "Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Downloading azure_mgmt_core-1.5.0-py3-none-any.whl (30 kB)\n",
      "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: azure-common, oauthlib, isodate, requests-oauthlib, azure-core, msrest, azure-mgmt-core, azure-cognitiveservices-search-imagesearch\n",
      "Successfully installed azure-cognitiveservices-search-imagesearch-2.0.1 azure-common-1.1.28 azure-core-1.32.0 azure-mgmt-core-1.5.0 isodate-0.7.2 msrest-0.7.1 oauthlib-3.2.2 requests-oauthlib-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Make sure we've got the latest version of fastai:\n",
    "!pip install -Uqq fastai\n",
    "!pip install azure-cognitiveservices-search-imagesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import os\n",
    "import shutil\n",
    "import fastai\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"a54e22634b134d548616088105f5c070\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(urls, save_folder='downloaded_images'):\n",
    "    # Create a directory to save images if it doesn't exist\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        try:\n",
    "            # Get the image content from the URL\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()  # Ensure we handle bad responses\n",
    "\n",
    "            # Determine the image extension based on Content-Type header\n",
    "            content_type = response.headers['Content-Type']\n",
    "            if 'image' in content_type:\n",
    "                extension = content_type.split('/')[-1]\n",
    "            else:\n",
    "                print(f\"Skipping non-image URL: {url}\")\n",
    "                continue\n",
    "\n",
    "            # Create the file path for saving the image\n",
    "            image_path = os.path.join(save_folder, f'image_{i}.{extension}')\n",
    "\n",
    "            # Save the image in chunks\n",
    "            with open(image_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            print(f\"Image {i+1} downloaded successfully: {image_path}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading image {i+1} from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_images_bing_rest(key, term, count=50, offset=0):\n",
    "    \"\"\"Search for images using Bing API with a direct REST call\"\"\"\n",
    "    endpoint = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": key}\n",
    "    params = {\n",
    "        \"q\": term,\n",
    "        \"count\": count,\n",
    "        \"offset\": offset,\n",
    "        \"minHeight\": 128,\n",
    "        \"minWidth\": 128,\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        if \"value\" in results:\n",
    "            return [img[\"contentUrl\"] for img in results[\"value\"]]\n",
    "        else:\n",
    "            print(\"No images found.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"Error occurred: {response.status_code} - {response.text}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_types = \"Centrifugal Pump\", \"Centrifugal Fan\", \"Robot\"\n",
    "path = Path(\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing directory if it already exists\n",
    "if path.exists():\n",
    "    shutil.rmtree(path)\n",
    "    \n",
    "# Create a fresh directory for storing images\n",
    "path.mkdir()\n",
    "\n",
    "# Iterate through each asset type\n",
    "for o in asset_types:\n",
    "    # Create a subdirectory for each asset type\n",
    "    dest = (path / o)\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Search Bing for images using the asset type + \"in Factory\" as query\n",
    "    # Retrieve up to 150 images per asset type\n",
    "    results = search_images_bing_rest(key, f'{o} in Factory', count=150)\n",
    "    \n",
    "    # Download the found images and save them in the corresponding asset type folder\n",
    "    download_images(results, save_folder=path/o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all image files from the specified directory\n",
    "fns = get_image_files(path)\n",
    "\n",
    "# Display the list of found image files\n",
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get failed images\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m failed \u001b[38;5;241m=\u001b[39m verify_images(\u001b[43mfns\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display failed images\u001b[39;00m\n\u001b[0;32m      5\u001b[0m failed\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fns' is not defined"
     ]
    }
   ],
   "source": [
    "# Get failed images\n",
    "failed = verify_images(fns)\n",
    "\n",
    "# Display failed images\n",
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data structure for training\n",
    "assets = DataBlock(\n",
    "    # Specify input (images) and output (categories) data types\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    \n",
    "    # Use get_image_files function to load the images\n",
    "    get_items=get_image_files,\n",
    "    \n",
    "    # Split data into training (80%) and validation (20%) sets with fixed random seed\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    \n",
    "    # Use the parent folder name as the category label\n",
    "    get_y=parent_label,\n",
    "    \n",
    "    # Transform all images to 128x128 pixels\n",
    "    item_tfms=Resize(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders from the DataBlock, which will handle batching and loading of images during training\n",
    "dls = assets.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a batch of images from the validation set\n",
    "# Show up to 4 images in a single row to visually inspect the data\n",
    "dls.valid.show_batch(max_n=4, nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataBlock with new image transformations\n",
    "assets = assets.new(\n",
    "    # Apply random resized cropping to 224x224 pixels, allowing crops as small as 50% of original\n",
    "    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n",
    "    # Disable any batch-level transformations\n",
    "    batch_tfms=None)\n",
    "\n",
    "# Create new data loaders with the updated transformations\n",
    "dls = assets.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN model using ResNet18 architecture, pre-trained on ImageNet\n",
    "# Use error rate as the evaluation metric\n",
    "learn = cnn_learner(dls, resnet18, metrics=error_rate)\n",
    "\n",
    "# Fine-tune the model for 4 epochs\n",
    "# This uses fastai's recommended fine-tuning approach:\n",
    "# 1. First trains only the new head layers\n",
    "# 2. Then gradually unfreezes and trains the entire network\n",
    "learn.fine_tune(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interpretation object to analyze model performance\n",
    "# This computes predictions on the validation set for analysis\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Generate and display a confusion matrix\n",
    "# Shows how well the model distinguishes between different asset classes\n",
    "# Helps identify which classes are commonly confused with each other\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 5 validation set images with highest losses\n",
    "# Shows 4 rows of images with their predicted and actual labels\n",
    "# Uses a 10x10 figure size for better visibility\n",
    "interp.plot_top_losses(5, nrows=4, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "learn.export(\"asset_classification.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
